# Reading

## Part1

### Reference

+ [知乎](https://zhuanlan.zhihu.com/p/37963267)

准确估计CTR和CVR，业界常用方法如下：

+ 人工特征+LR
+ GBDT+LR
+ FM
+ FFM

在进行CTR预估时，除了单特征之外，还需要对特征进行组合。对于特征组合，主要有两大类：**FM**系列和**Tree**系列。

### 目的

+ 稀疏数据特征组合

### 优势

+ 对稀疏数据进行组合
+ 线性的计算复杂度

### 特征组合

在对特征进行编码时，会遇到许多category类型的特征，比如国家，性别，职业，教育水平等等，这些特征在编码成向量的时候需要进行One-Hot编码，这样，大部分的样本数据都是比较稀疏的，这个就是特征的**稀疏性**。

某些特征经过关联之后，与label之间的相关性就会提高，而且，通常情况下，特征之间的关联性也比较大，因此，引入特征组合是非常有意义的。

#### 特征组合方式

多项式模型是包含特征组合的最直观的模型。在多项式模型中，特征$x_i$和$x_j$的组合采用$x_ix_j$表示，通俗理解来看，当$x_i$和$x_j$都为非零的时候才会有意义。模型表达式如下
$$
y(\mathbf{X})=w_0+\sum^n_{i=1}w_ix_i+\sum^{n-1}_{i=1}\sum^n_{j=i+1}w_{ij}x_ix_j
$$
其中，$n$表示样本的特征数量，$x_i$表示第$i$个特征的值，$w_0,w_i,w_{ij}$是模型的参数。

在数据稀疏性普遍存在的实际应用的场景中，二次项系数训练由于需要$x_i$和$x_j​$都为非零样本，但由于样本本来就比较稀疏，所以满足两者都为非零的样本会很少，训练会很困难。

#### 二次项参数训练

可以将$W$矩阵分解为$W=V^TV$，换句话说，特征向量$x_i$和$x_j$的交叉项系数就等于$x_i$对应的隐向量与$x_j$对应的隐向量之间的内积，即每个参数$w_{ij}=<v_i,v_j>$。对$W$分解如下
$$
W^*=
\left[
\begin{matrix}
w_{11} & w_{12} & ... & w_{1n} \\
w_{21} & w_{22} & ... & w_{2n} \\
... & ... & ...& ... \\
w_{n1} & w_{n2} & ... & w_{nn}
\end{matrix}
\right]
=V^TV=
\left[
\begin{matrix}
v_{11} & v_{12} & ... & v_{1k} \\
v_{21} & v_{22} & ... & v_{2k} \\
... & ... & ...& ... \\
v_{n1} & v_{n2} & ... & v_{nk}
\end{matrix}
\right]
\times
\left[
\begin{matrix}
v_{11} & v_{21} & ... & v_{n1} \\
v_{12} & v_{22} & ... & v_{n2} \\
... & ... & ...& ... \\
v_{1n} & v_{2n} & ... & v_{nk}
\end{matrix}
\right]
$$

> $W$上对角的元素就是交叉项的参数。

FM的模型方程为
$$
\hat{y}(X):=w_0+\sum^n_{i=1}w_ix_i+\sum^{n-1}_{i=1}\sum^n_{j=i+1}<v_i,v_j>x_ix_j
$$
其中
$$
<v_i,v_j>:=\sum^k_{f=1}v_{i,f}\cdot{v_{i,f}}
$$
表示隐向量的内积。

> 这种方法极大地减少了参数量，将参数量减少为$kn$个。另外可以理解为，原始的$w_{ij}$之间相互独立，在学习$w_{ij}$的时候只能使用$x_i$和$x_j$都为非零的样本，但是使用隐向量之后，所有包含$x_i$的非零组合特征都可以用来学习隐向量$v_i$，这样很大程度上减小了数据稀疏性造成的影响。

#### 训练损失函数

+ MSE损失函数解决回归问题
+ Hinge/Cross-Entropy解决分类问题
+ Sigmoid变换解决二分类问题

#### FM二次项化简

可以将FM的二次项进行化简，化简到$O(kn)$。

推导如下
$$
\begin{equation}
\begin{aligned}
\sum^{n-1}_{i=1}\sum^n_{j=i+1}<v_i,v_j>x_ix_j 
&=\frac{1}{2}\sum^n_{i=1}\sum^n_{j=1}<v_i,v_j>x_ix_j-\frac{1}{2}\sum^n_{i=1}<v_i,v_i>x_ix_i \\
&=\frac{1}{2}(\sum^n_{i=1}\sum^n_{j=1}\sum^k_{f=1}v_{i,f}v_{j,f}x_ix_j-\sum^n_{i=1}\sum^k_{f=1}v_{i,f}v_{i,f}x_ix_i) \\
&=\frac{1}{2}\sum^k_{f=1}[(\sum^n_{i=1}v_{i,f}x_i)\cdot{(\sum^n_{j=1}v_{j,f}x_j})-\sum^n_{i=1}v^2_{i,f}x^2_i] \\
&=\frac{1}{2}\sum^k_{f=1}[(\sum^n_{i=1}v_{i,f}x_i)^2-\sum^n_{i=1}v^2_{i,f}x^2_i]
\end{aligned}
\end{equation}
$$

#### 梯度下降求解参数

采用随机梯度下降法SGD求解参数
$$
\frac{\partial\hat{y}(x)}{\partial{\theta}}=
\begin{cases}
1, & \theta=w_0 \\
x_i, & \theta=w_i \\
x_i\sum^n_{j=1}v_{j,f}x_j-v_{i,f}x^2_i, & \theta=v_{i,f}
\end{cases}
$$

### 关于隐向量的理解

这里的$v_i$相当于是$x_i$的低纬度稠密表达，实际中，隐向量的长度通常远小于特征维度$N$。

## Part2

在FFM模型中，每一维的特征$x_i$，针对其他特征的每一种field$f_j$，都会学习一个隐向量$V_{i,f_j}$。因此，隐向量不仅与特征有关，也与field相关。

### 算法原理

假设样本一共有$n$个特征，$f$个field，那么FFM的二次项有$nf$个隐向量。相当于每一个特征在每一个field里面都有一个隐含的向量。由此导出模型方程为
$$
y=w_0+\sum^n_{i=1}w_ix_i+\sum^n_{i=1}\sum^n_{j=i+1}<V_{i,f_j},V_{j,f_i}>x_ix_j
$$
其中，$f_j$是第$j$的特征所属的字段。